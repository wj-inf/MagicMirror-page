<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        h1 {
            color: #2c3e50;
            text-align: center;
        }
        .authors {
            text-align: center;
            margin-bottom: 30px;
        }
        .affiliations {
            text-align: center;
            margin-bottom: 30px;
        }
        .contact {
            text-align: center;
            margin-top: 30px;
            padding-top: 20px;
            border-top: 1px solid #eee;
        }
        .content-image {
            max-width: 100%;
            height: auto;
        }
        .links {
            text-align: center;
            margin-bottom: 40px;
        }
        .links a {
            margin: 0 5px; /* Add some space between badges */
            text-decoration: none; /* This line removes the underline */
            vertical-align: middle; /* Ensures badges align perfectly */
        }
        figcaption {
            text-align: center;
        }
    </style>
</head>
<body>
    <h1>MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation</h1>
    
    <div class="authors">
        <p>
            Jia Wang<sup>1,2</sup> &nbsp;&nbsp; Jie Hu<sup>2</sup><sup>&dagger;</sup> &nbsp;&nbsp; Xiaoqi Ma<sup>2</sup> &nbsp;&nbsp; Hanghang Ma<sup>2</sup> &nbsp;&nbsp; Yanbing Zeng<sup>2</sup> &nbsp;&nbsp; Xiaoming Wei<sup>2</sup>
        </p>
    </div>
    
    <div class="affiliations">
        <p>
            <sup>1</sup>University of Chinese Academy of Sciences &nbsp;&nbsp;
            <sup>2</sup>Meituan &nbsp;&nbsp;
        </p>
        <p>
            <sup>&dagger;</sup>Corresponding author</p>
        </p>
    </div>

    <!-- === MODIFIED LINKS SECTION === -->
    <div class="links">
        <a href="https://arxiv.org/abs/2509.10260">
            <img src="https://img.shields.io/badge/arXiv-2509.10260-b31b1b.svg" alt="arXiv">
        </a>
        <a href="https://huggingface.co/datasets/wj-inf/MagicData340k">
            <img src="https://img.shields.io/badge/ðŸ¤—%20Huggingface-Dataset-yellow" alt="Dataset on Hugging Face">
        </a>
        <a href="https://huggingface.co/wj-inf/MagicAssessor-7B">
            <img src="https://img.shields.io/badge/ðŸ¤—%20Huggingface-Model-blue" alt="Model on Hugging Face">
        </a>
        <a href="https://github.com/wj-inf/MagicMirror">
            <img src="https://img.shields.io/badge/GitHub-Benchmark-181717?logo=github" alt="GitHub Repository">
        </a>
        <a href="https://wj-inf.github.io/MagicMirror-page">
            <img src="https://img.shields.io/badge/Home-Page-b3.svg" alt="Page">
        </a>
    </div>
    <!-- === END OF MODIFIED SECTION === -->

    <div class="abstract">
        <h2>Abstract</h2>
        <p>Text-to-image (T2I) generation has achieved remarkable progress in instruction following and aesthetics. However, a persistent challenge is the prevalence of physical artifacts, such as anatomical and structural flaws, which severely degrade perceptual quality  and limit application. Given the diversity and complexity of these artifacts, a systematic and fine-grained evaluation framework is required, which is lacking in current benchmarks. To fill this gap, we introduce <strong>MagicMirror</strong>, a comprehensive framework for artifacts assessment. We first establish a detailed taxonomy of generated image artifacts. Guided by this taxonomy, we manually annotate <strong>MagicData340K</strong>, the first human-annotated large-scale dataset of 340K generated images with fine-grained artifact labels. Building on this dataset, we train <strong>MagicAssessor</strong>, a Vision-Language Model (VLM) that provides detailed assessments and corresponding labels. To overcome challenges like class imbalance and reward hacking, we design a novel data sampling strategy and a multi-level reward system for Group Relative Policy Optimization (GRPO). Finally, we leverage MagicAssessor to construct <strong>MagicBench</strong>, an automated benchmark for evaluating the image artifacts of current T2I models. Our evaluation with MagicBench reveals that despite their widespread adoption, even top-tier models like GPT-image-1 are consistently plagued by significant artifacts, highlighting artifact reduction as a critical frontier for future T2I development.</p>
    </div>
    
    <div class="task-intro">
        <h2>Task Intro</h2>
        <figure>
            <img src="images/Fig_Model_Response_exp.png" alt="Assessment Example" class="content-image">
            <figcaption>Figure 1: An output example of MagicAssessor-7B.</figcaption>
        </figure>
        <p>Given an image generated by an AIGC model and its corresponding prompt, our model can detect artifacts that appear in the image. In this example, an artifact in the car's rearview mirror is accurately identified.</p>
    </div>
    <div class="classification">
        <h2>Classification of Artifacts</h2>
        <figure>
            <img src="images/Fig_artifacts_classification_mindmap.png" alt="Artifacts Classification Mindmap" class="content-image">
            <figcaption>Figure 2: The hierarchical taxonomy of image artifacts.</figcaption>
        </figure>
        <p>We define Normal/Artifact as Level 1 (L1). At the highest level, we distinguish between artifacts concerning the subject itself and those involving interactions between subjects. Subject-level issues are further divided into Anatomy (including human, animal, and object structure) and Attributes (including color and proportion). These main categories constitute our Level 2 (L2) labels. For critical areas, we define more specific Level 3 (L3) labels, such as <em>Hand Structure Deformity</em>.</p>
        <figure>
            <img src="images/Fig_Four_L2_Label_Example.png" alt="L2 Label Examples" class="content-image">
            <figcaption>Figure 3: Visual examples of artifacts corresponding to our taxonomy.</figcaption>
        </figure>
    </div>
    <div class="data-curation">
        <h2>Data Curation</h2>
        <figure>
            <img src="images/Fig_Data_Curation.png" alt="Data Curation" class="content-image">
            <figcaption>Figure 4: Data curation for MagicData340K.</figcaption>
        </figure>
        <p>The process begins with curating prompts from diverse sources and generating corresponding images with a suite of T2I models. A comprehensive annotation taxonomy for artifacts is then developed through iterative tests and applied to the resulting text-image pairs. Finally, a representative subset is selected for fine-grained annotation, where detailed descriptions for each label are written by humans and used to synthesize Chain-of-Thought (CoT) rationales with GPT-4o.</p>
    </div>
    <div class="training-strategy">
        <h2>Training Strategy</h2>
        <figure>
            <img src="images/Fig_Training_Pipeline.png" alt="Training Strategy" class="content-image">
            <figcaption>Figure 5: Data sampling strategy and reward system in model training.</figcaption>
        </figure>
        <p>To enhance its performance, we employ GRPO, which we uniquely adapt to our task through two key innovations: a targeted data sampling strategy and a multi-level reward system. First, to address the issue of data imbalance, our data sampling strategy oversamples challenging positive cases, such as anatomically correct hands. Then, we propose a multi-level reward system, which guides the model from coarse to fine-grained detection and introduce a novel consistency reward to align the model's reasoning with its final output to prevent reward hacking.</p>
    </div>
    <div class="overall-performance">
        <h2>Overall Performance</h2>
        <figure>
            <img src="images/table1_overall_performance.png" alt="Overall Performance" class="content-image">
            <figcaption>Table 1: Overall performance comparison with other models.</figcaption>
        </figure>
        <p>Our model demonstrates strong performance in both detailed artifact detection and overall quality assessment. Compared to existing VLM-based assessors, MagicAssessor achieves the best overall performance, especially excelling in detailed evaluation. It also shows a high correlation with human preferences, outperforming both traditional metrics like CLIP Score and other VLM-based models.</p>
    </div>
    <div class="benchmark-results">
        <h2>Benchmark Results</h2>
        <figure>
            <img src="images/table3_benchmark.png" alt="Benchmark Results" class="content-image">
            <figcaption>Table 2: Evaluation score (â†‘) on different labels of different models in MagicBench.</figcaption>
        </figure>
        <p>We evaluate various state-of-the-art T2I models using MagicBench. The results show that while some models excel in specific areas, MagicAssessor provides a more balanced and comprehensive assessment. Our benchmark reveals critical insights into the current state of T2I models and highlights the importance of fine-grained artifact evaluation.</p>
    </div>

    
    <div class="citation">
        <h2>Citation</h2>
        <pre><code>@article{wang2025magicmirror,
    title   = {MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation},
    author  = {Wang, Jia and Hu, Jie and Ma, Xiaoqi and Ma, Hanghang and Zeng, Yanbing and Wei, Xiaoming},
    journal = {arXiv preprint arXiv:2509.10260},
    year    = {2025}
    }</code></pre>
        </div>
        
</body>
</html>